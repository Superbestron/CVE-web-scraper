import requests
from bs4 import BeautifulSoup
import pandas as pd
import math

# Categories available to be scraped on the website
NO_OF_CATEGORIES_1 = 16 # If user searches for vendor and product, all categories of data will be shown
NO_OF_CATEGORIES_2 = 15 # If user searches for vendor but doesn't search for product, product data won't be shown
NO_OF_CATEGORIES_3 = 16 # If user searches for product but doesn't search for vendor, product and vendor data will still be shown
NO_OF_CATEGORIES_4 = 14 # If user doesn't search for vendor and product, both data won't be shown
categories_1 = {1: 'Vendor', 2: 'Product', 3: '1-CVE ID', 4: 'CWE ID', 5: 'Vulnerability Type', 6: 'Publish Date', 7: 'Update Date', 8: 'CVSS Score', 9: 'Gained Access Level', 10: 'Access', 11: 'Complexity', 12: 'Authentication', 13: 'Configuration', 14: 'Integrity', 15: 'Availability', 16: 'Link'}
categories_2 = {1: 'Vendor', 2: '1-CVE ID', 3: 'CWE ID', 4: 'Vulnerability Type', 5: 'Publish Date', 6: 'Update Date', 7: 'CVSS Score', 8: 'Gained Access Level', 9: 'Access', 10: 'Complexity', 11: 'Authentication', 12: 'Configuration', 13: 'Integrity', 14: 'Availability', 15: 'Link'}
categories_3 = {1: 'Vendor', 2: 'Product', 3: '1-CVE ID', 4: 'CWE ID', 5: 'Vulnerability Type', 6: 'Publish Date', 7: 'Update Date', 8: 'CVSS Score', 9: 'Gained Access Level', 10: 'Access', 11: 'Complexity', 12: 'Authentication', 13: 'Configuration', 14: 'Integrity', 15: 'Availability', 16: 'Link'}
categories_4 = {1: '1-CVE ID', 2: 'CWE ID', 3: 'Vulnerability Type', 4: 'Publish Date', 5: 'Update Date', 6: 'CVSS Score', 7: 'Gained Access Level', 8: 'Access', 9: 'Complexity', 10: 'Authentication', 11: 'Configuration', 12: 'Integrity', 13: 'Availability', 14: 'Link'}

# Track if searching by vendor and product
vendor_input_yes = False
product_input_yes = False
URL = 'https://www.cvedetails.com/vulnerability-search.php?f=1'

# 1989-12 to 2019-11 contains all search results
NO_OF_MONTHS = 250 # From 1999-01 to 2019-11
MAX_NO_OF_SEARCHES = 200
MAX_CVSS_SCORE = 10

def get_user_input():
    print('Welcome to CVE search!')

    vendor_input = input('Enter the vendor name: ')
    if vendor_input:
        global vendor_input_yes
        vendor_input_yes = True
    vendor = '&vendor=' + vendor_input

    product_input = input('Enter the product name: ')
    if product_input:
        global product_input_yes
        product_input_yes = True
    product = '&product=' + product_input

    global URL
    URL = vendor + product
    print('\nAny particular vulnerabilities you are searching for? (You may select more than one)')
    print('1 for Bypass a restriction or similar\n' 
          '2 for Cross Site Scripting\n'              
          '3 for Denial of service\n'
          '4 for Directory Traversal\n'
          '5 for Execute arbitrary code on vulnerable system\n'
          '6 for Gain Privileges\n'
          '7 for Http Response Splitting\n'
          '8 for Memory Corruption\n' 
          '9 for Obtain information\n'
          '10 for Overflow vulnerability (includes stack and heap based overflows and other overflows)\n'
          '11 for Cross site request forgery(CSRF)\n'
          '12 for File Inclusion\n'
          '13 for Sql Injection\n'
          'Press Enter to proceed')
    print()
    vuln_list = {1: '&opbyp=1', 2: '&opxss=1', 3: '&opdos=1', 4: '&opdirt=1', 5: '&opec=1', 6: '&opgpriv=1', 7: '&ophttprs=1', 8: '&opmemc=1', 9: '&opginf=1', 10: '&opov=1', 11: '&opcsrf=1', 12: '&opfileinc=1', 13: '&opsqli=1'}
    while True:
        choice = input('Enter your search (again) - Type only a number per search: ')

        if (len(choice) == 0):
            return URL
        URL = URL + vuln_list[int(choice)] 

# This function allows user to choose which categories of data he wants to omit
def data_desired(data_list, categories):    
    print('Choose which categories of data you want to omit in the exported CSV. (Default is none)')
    for index in categories:
        print (index, ':', categories[index])
    print('Press Enter to proceed\n')
    choice = [int(x) for x in input("Enter multiple values each separated by a space: ").split()] # 10 12 8 for eg
  
    if (len(choice) == 0):
        return data_list

    choice.sort(reverse=True) # Reverse the choices so later on I will delete the data with higher indexes, hence not screwing up the deletion of data

    # Okay this step can seem a little confusing, so first I find the first index of 'choice'
    # This value will correspond to its name in categories
    # Lastly, I will delete the key, with its corresponding data 

    for i in range(len(choice)):
        del data_list[categories[choice[i]]]
    return data_list

def get_data(search_URL):
    page = requests.get(search_URL)

    soup = BeautifulSoup(page.content, 'html.parser')
    results = soup.find(id='vulnslisttable') 

    vuln_elems = results.find_all('tr', class_='srrowns')
    return vuln_elems

# This function pulls out details from each vulnerability from each webpage and returns data_list appended with the new search data
def scrape_data(vuln_elems, data_list):
    for vuln_elem in vuln_elems:
        vuln_details = vuln_elem.find_all('td')
        del vuln_details[len(vuln_details) - 12] # Remove index 5, and its always 12 less than the length
        del vuln_details[0]
        for index, vuln_detail in enumerate(vuln_details, start=1):
            data_list[categories[index]].append(vuln_detail.text.strip())

        # Link is found in CVE-ID category which is always 14 less than the length
        data_list[categories[NO_OF_CATEGORIES]].append(vuln_details[NO_OF_CATEGORIES - 14].find('a')['href'])       
    return data_list   





##### WHERE PROGRAM STARTS #####

URL = URL + get_user_input()
search_URL = URL # save a copy of the original URL so it's not altered later in the program
print('Currently scraping: ' + URL)
print()       

if vendor_input_yes:
    if product_input_yes:
        NO_OF_CATEGORIES = NO_OF_CATEGORIES_1
        categories = categories_1
        #print('categories_1')
    else:
        NO_OF_CATEGORIES = NO_OF_CATEGORIES_2
        categories = categories_2
        #print('categories_2')
else:
    if product_input_yes: 
        NO_OF_CATEGORIES = NO_OF_CATEGORIES_3
        categories = categories_3
        #print('categories_3')
    else:
        NO_OF_CATEGORIES = NO_OF_CATEGORIES_4
        categories = categories_4
        #print('categories_4')

# Creates a dictionary of 16 keys to store all my 16 categories of data
# Each entry in data_list will contain the name of the key from categories
# E.g. data_list --> {'Vendor': [], 'Product': [], ...}

data_list = {}        
for i in range(1, NO_OF_CATEGORIES + 1):
    data_list[categories[i]] = []

# If the usual search gives > 200 results, we need to search with more specific details, in this case we filter by cvss score first
if len(get_data(search_URL)) == MAX_NO_OF_SEARCHES:
    # check by cvss score in ranges of 0.5
    for k in range((2 * MAX_CVSS_SCORE) + 1):
        search_URL = URL + '&cvssscoremin={0}&cvssscoremax={1}'.format(k/2, k/2 + 0.4)
        
        # If search by cvss score purely still gives > 200 results, then need to search by date also
        if len(get_data(search_URL)) == MAX_NO_OF_SEARCHES:
            print('This might take more than an hour...')

            # search by cvss score and date (total 250 months x 19 0.5-ranges = 4750 pages) 
            # so this program is gonna try to avoid to avoid as much as possible by first testing if just cvss score search filter will do the job as coded below
            # anyway for this case we are assuming the max. no. of search results < 200, for the large amount of cases this is true
            # but I haven't tested this out extensively, and increasing the number of search filters to recitfy this problem would complicate the code a lot
            for i in range((2 * MAX_CVSS_SCORE) + 1):                
                 # We want to search from 1999-01 to 2019-11 (total 250 months)
                 # 0.5-0.9 and 9.5-9.9 no vulns so we not gonna search that range
                 print('Scraping from CVSS Score {0}...'.format(i/2))
                 if (i != 1) and (i != 19): # 1 corresponds to 0.5 and 19 to 9.5 cvss score   
                     for j in range(NO_OF_MONTHS): 
                         search_URL = URL + '&cvssscoremin={0}&cvssscoremax={1}'.format(i/2, i/2 + 0.4)
                         search_URL = search_URL + '&psy={0}&psm={1}&pey={2}&pem={3}'.format(math.floor(1999 + j/12), (j%12) + 1, math.floor(1999 + (j+1)/12), ((j+1) % 12) + 1)    
                         vuln_elems = get_data(search_URL)    
                         data_list = scrape_data(vuln_elems, data_list)

            # It is computationally intensive to iterate through 19 x 12 = 360 searches per year
            # So I just hard-coded these search ranges to reduce the number of total iterations --> the arrow shows no. of results (all < 200)
            # 1989-12 to 1997-1 --> 182, 1997-1 to 1997-10 --> 194, 1997-10 to 1998-08 --> 177, 1998-08 to 1999-01 --> 96

            search_URL = URL + '&psy=1989&psm=12&pey=1997&pem=1'   
            vuln_elems = get_data(search_URL)    
            data_list = scrape_data(vuln_elems, data_list)
            search_URL = search_URL + '&psy=1997&psm=1&pey=1997&pem=10'  
            vuln_elems = get_data(search_URL)    
            data_list = scrape_data(vuln_elems, data_list)
            search_URL = search_URL + '&psy=1997&psm=10&pey=1998&pem=8'    
            vuln_elems = get_data(search_URL)    
            data_list = scrape_data(vuln_elems, data_list)
            search_URL = search_URL + '&psy=1998&psm=8&pey=1999&pem=1'   
            vuln_elems = get_data(search_URL)    
            data_list = scrape_data(vuln_elems, data_list)
            break # breaks out of outer-for loop

        else:
            # search by cvss score
            print('This might take up a few minutes!')
            for i in range(2 * MAX_CVSS_SCORE):
                search_URL = URL + '&cvssscoremin={0}&cvssscoremax={1}'.format(i/2, i/2 + 0.5)
                vuln_elems = get_data(search_URL)    
                data_list = scrape_data(vuln_elems, data_list)           
            break # breaks out of outer-for loop
   
else:
    print('This will be quick!')
    vuln_elems = get_data(search_URL)    
    data_list = scrape_data(vuln_elems, data_list)

df = pd.DataFrame(data_desired(data_list, categories))
df.to_csv('CVE_DATABASE.csv', index=False, encoding='utf-8')
print('File created!')
